* Neural Network Papers

This is a the notes of papers I read about neural network.

** Understanding the difficulty of training deep feed-forward neural networks

*** Assumptions

Implicit assumptions for its theoretical derivation is:

1. Input of the network has expectation of zero.
2. All random variables, i.e., the weights, derivatives and etc.,
   have expectation of zero.

The reason for the above is that:

Var(XY) = Var(X) * Var(Y) + Var(X) * E(Y)**2 + Var(Y) * E(X) ** 2

For independent X, Y, in order to obtain:

Var(XY) = Var(X) * Var(Y)

We need E(X) = E(Y) = 0.

*** Normalized initialization

The weights with normalized initialization have a constant variance
irrelevant to the depth of the network.

** Efficient Backprop

Tricks for improving convergence.

1. Empirical risk minimization 40, 41
2. Decomposing the generalization error to two terms: bias and
   variance 12. It can be shown that the minimum total error will
   occur when the sum of bias and variance are minimal.
3. Advantages of stochastic training over batch training:
   1) stochastic training is usually much faster.
   2) stochastic training often results in better solutions.
   3) stochastic training can be used for tracking changes.
4. Advantages of batch training over stochastic training:
   1) conditions of convergence is well understood.
   2) many acceleration techniques only operate in batch mode.
   3) theoretical analysis of the weight dynamics and convergence
      rates are much simpler.
5. Shuffling the examples and choose examples with maximum
   information content.  But this might learn the features of the
   *data outliers*.
   1) Shuffle the training set so that successive training examples
      rarely belong to the same class
   2) Present input examples that produce a large error more
      frequently than examples that produce a small error.
6. Transforming the inputs.
   1) The average of each input variable over the training set should
      be close to zero.
   2) Scale input variables to that their convariance are about the same.
   3) Input variables should be uncorrelated if possible.
7. Sigmoid:
   1) Recommended sigmoid 19: f(x) = 1.7159 tanh(2/3 x)
   2) Choose target tangent values at the point of the maximum second
      derivative on the sigmoid so as to avoid saturating the output
      units.
8. Initializing weights.
   1) weights should be chosen *randomly* and in such way that the
      sigmoid is primarily *activated in its linear range*.
   2) randomly distributed with *mean zero* and *standard deviation*
      \sigma = m^{-1/2}, where m is the fan-in (the number of
      connections feeding into the node), assuming the inputs to a
      unit are uncorrelated with variance 1.
9. Equalize the learning speed.
   1) Give each weight its own learning rate.
   2) learning rate should be proportional to the square root of the
      number of inputs to the unit
   3) weights in lowers layers should typically be larger than in the
      higher layers.
10. Section 5, reasoning behind Hessian. Needs further experimenting.

** Accelerated Learning in Layered Neural Networks

Compare two error functions: quadratic and cross entropy (logarithm).

*** Conclusions

1. Logarithm does not improve the generalization ability of the
   resulting network.  Its advantage resides in systemic reductions
   in learning time, as large as an order of magnitude for large
   receptive field, /p/.
2. *Conjecture*: A larger density of local minima that are not global
   minima in quadratic error surface is responsible for the more
   frequent failure.  It's supported that learning constant for
   logarithm is much smaller than quadratic function.
3. It *remains open* that logarithm is always better than quadratic
   function.

*** Test Methods

1. Data set: *Contiguity problem*, which is a classification of
   binary input patterns I = (I_1, I_2, I_3, ..., I_n), I_i = 0, 1
   for all 1 <= i <= n, into classes according to the number k of
   blocks of +1's in the pattern.  For example, for N = 10, I =
   (0110011100) corresponds to k = 2, while I = (0101101111)
   corresponds to k = 3.  This classification leads to (1 + k_{max})
   categories corresponding to 0 <= k <= k_{max}, where k_{max}
   =ceil(N/2).  A simpler classification task used is the dichotomy
   into two classes corresponding to k < k_0 and k > k_0.

   Here, the author used k = 2 and k = 3, 330 and 462 total available
   patterns.  Out of which, 50 examples of each case is selected.
   Here N = 10.

2. Network parameters:

   1) 3-layer network, 10-10-1, the first being input layers.
   2) Network is not fully connected, each unit in layers 1 receives
      input from only the /p/ subsequent units below it. the parameter
      /p/ can be interpreted as the width of a receptive field.

3. Training error and generalization error are monitored.

#  LocalWords:  XY Backprop outliers convariance Sigmoid sigmoid tanh
#  LocalWords:  minima
